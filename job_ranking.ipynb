{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "pdf_path = \"Tenzin Norzin Resume.pdf\"#add resume in pdf format\n",
    "text = pdf_to_text(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENZIN EDUCATION B Tech Hons Computer Pandas Supervised Unsupervised Deep Mathematical Foundations Statistics Probability Algebra Calculus Matrices Database Management MySQL NoSQL MongoDB Programming Languages C R CE IFICATIONS DSA LeetCode Exercises Intermediate course SkillsBuild openHPI PROJECTS Psychbot Intern Initiated blogs informative posts developed strategies products Managed inventory items including conducting market Bug Clasher Shoot Bug Quiz Craze Developing driven job portal disabled students leveraging deep enhance essibility personalized job matching course Designed based recommendation system utilizes distinct filters MRI scans assess predict stages Alzheimer s disease aiming facilitate earlier intervention enhance patient outcomes Developed theoretical offline bus ticket payment system enable digital transactions areas limited internet connectivity DSA LeetCode Exercises techniques analyze user provided predicting mental health outcomes providing advice suggestions appropriate help needed Bug Clasher Shoot Bug Quiz Craze Badges Artificial Intelligence Innovative Communication Innovative Collaborative Versatility Leadership Hindi English Hindi Tibetan\n",
      "1186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "#preprocessing\n",
    "import re\n",
    "import spacy\n",
    "    \n",
    "def preprocess_resume(text):\n",
    "        #removing contacts info\n",
    "    text = re.sub(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",'', text)\n",
    "    text = re.sub(r\"\\+?\\d{1,3}[-.\\s]?\\(?\\d{2,3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\",'', text)\n",
    "        \n",
    "        #removing URLs\n",
    "    text = re.sub(r'http\\S+\\s*', '', text)\n",
    "        \n",
    "        #removes common Twitter related terms\n",
    "    text = re.sub('RT|cc', ' ', text)\n",
    "        \n",
    "        #removing hashtags\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "        \n",
    "        #removing mentions\n",
    "    text = re.sub(r'@\\S+', ' ', text)\n",
    "        \n",
    "        #removing punctuations\n",
    "    text = re.sub('[%s]' % re.escape(r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_{|}~\"\"\"), ' ', text)\n",
    "        \n",
    "        #removing Non-ASCII Characters\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r' ', text)\n",
    "        \n",
    "        #remove social media mentions\n",
    "    text = re.sub(r'\\b(linkedin|github|coursera|kaggle|hackerank|location)\\b', '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "    month_pattern = r\"\\b(?:January|February|March|April|May|June|July|August|September|October|November|December|month|year)s?\\b\"\n",
    "    text = re.sub(month_pattern, '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        #removing extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\"])\n",
    "    text = text\n",
    "    doc = nlp(text) \n",
    "        \n",
    "        #removing name and location\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            names = ent.text.split()\n",
    "            pattern = r'\\b(?:' + '|'.join(map(re.escape, names)) + r')\\b'\n",
    "            text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "                \n",
    "        elif ent.label_ in {\"LOC\", \"GPE\", \"ORG\"}:\n",
    "            loca = ent.text.split()\n",
    "            pattern_loc = r'\\b(?:' +'|'.join(map(re.escape, loca)) + r')\\b'\n",
    "            text = re.sub(pattern_loc, '', text, flags=re.IGNORECASE)\n",
    "            \n",
    "        # Remove dates in various formats (MM/DD/YYYY, YYYY-MM-DD, etc.)\n",
    "    date_pattern = r\"\\b(?:\\d{1,2}[-/\\.]?\\d{1,2}[-/\\.]?\\d{2,4}|\\d{4}[-/\\.]?\\d{1,2}[-/\\.]?\\d{1,2}|\\d{1,2}\\s+[A-Za-z]+\\s+\\d{4})\\b\"\n",
    "    text = re.sub(date_pattern, '', text)\n",
    "        \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "        #text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        #label encoder\n",
    "        #tokenization & stop word removal\n",
    "    doc = nlp(text)\n",
    "    text = [token.text for token in doc if not token.is_stop]\n",
    "    text = ' '.join(text)\n",
    "        \n",
    "    return text\n",
    "    \n",
    "    '''text = \"Atomcode is Hiring C Data Analysts c (Work From Home) C++ \\n We are hiring talented and enthusiastic Data Analysts for an immediate joiner position. This is a work-from-home opportunity!\\n Requirements:\\n Proficiency in Excel, SQL, and data visualization tools like Power BI or Tableau.\\n Strong analytical and problem-solving skills.\\n Knowledge of data cleaning, preprocessing, and statistical analysis.\\n Familiarity with Python or R for data analysis is a plus.\\n Join us and make an impact with your skills!\"\n",
    "    a = text.split()\n",
    "    print(len(a))'''\n",
    "cleaned_text = preprocess_resume(text)\n",
    "print(cleaned_text)\n",
    "print(len(cleaned_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Academic Counsellor at EdiGlobe Online Services Pvt Ltd->match:0.623\n",
      "2. Business Development Associate at Intellipaat (E- Learning)->match:0.595\n",
      "3. Program Architect at Kalvium (Engineer)->match:0.587\n",
      "4. Business Development Associate at KnoDTec Solutions->match:0.577\n",
      "5. Software engineer at Prodapt->match:0.574\n",
      "6. Business Development Intern at Terratern->match:0.544\n",
      "7. data analyst at Atom Code->match:0.526\n",
      "8. Solar Consultant at SolarSquare Energy Pvt Ltd->match:0.515\n",
      "9. Lead Generation Executive at Skill Intern Pvt Ltd->match:0.475\n",
      "10. Graduate Trainee Engineer at Adtran->match:0.452\n",
      "11.  at ->match:0.036\n",
      "12.  at ->match:0.036\n",
      "13.  at ->match:0.036\n",
      "14.  at ->match:0.036\n"
     ]
    }
   ],
   "source": [
    "#Implementing Cosine Similarity in Python (TF-IDF Approach)\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "file_path = r\"C:\\Users\\HP\\OneDrive\\Documents\\Projects\\final year project\\job description\\job description dataset.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "job_descriptions = df[\"job_description\"].to_list()\n",
    "company_names = df[\"company name\"].to_list()\n",
    "job_titles = df[\"job_title\"].to_list()\n",
    "\n",
    "resume = cleaned_text\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "job_embeddings = model.encode(job_descriptions)\n",
    "resume_embedding = model.encode(resume)\n",
    "\n",
    "similarity_scores = cosine_similarity([resume_embedding], job_embeddings)[0]\n",
    "\n",
    "ranked_jobs = sorted(enumerate(similarity_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for rank, (index, score) in enumerate(ranked_jobs, start=1):\n",
    "    print(f\"{rank}. {job_titles[index]} at {company_names[index]}->match:{score:.3f}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
